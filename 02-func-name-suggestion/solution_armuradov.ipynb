{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "5aeea738b38f42dc9e678a5e98ed8cb7": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_cd691d1f563d48c28a832d927c50e723",
              "IPY_MODEL_1ceed22c15614f869955fba038f538f7",
              "IPY_MODEL_7ffdbb1da10f4c46885315be7e0812e6"
            ],
            "layout": "IPY_MODEL_52d3f4823d2b4792ad5b5f0a3af6f80c"
          }
        },
        "cd691d1f563d48c28a832d927c50e723": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_5e0b7e05d1f1462cbbfb38e7d910079b",
            "placeholder": "​",
            "style": "IPY_MODEL_14ea3cab119c456b88b2334d1ceab03f",
            "value": "Map: 100%"
          }
        },
        "1ceed22c15614f869955fba038f538f7": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_c309c635e0a94d2097c3e96ee3952b84",
            "max": 1000,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_27f6875cfba343e38490984b88f98127",
            "value": 1000
          }
        },
        "7ffdbb1da10f4c46885315be7e0812e6": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_9e9b53fcd9e846a19834aa60372f4f50",
            "placeholder": "​",
            "style": "IPY_MODEL_865ee319021047438e0e459371f151db",
            "value": " 1000/1000 [00:02&lt;00:00, 275.21 examples/s]"
          }
        },
        "52d3f4823d2b4792ad5b5f0a3af6f80c": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "5e0b7e05d1f1462cbbfb38e7d910079b": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "14ea3cab119c456b88b2334d1ceab03f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "c309c635e0a94d2097c3e96ee3952b84": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "27f6875cfba343e38490984b88f98127": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "9e9b53fcd9e846a19834aa60372f4f50": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "865ee319021047438e0e459371f151db": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jZuQEVUZhDks",
        "outputId": "daffba16-dd72-416e-acc9-2ed643a2ec4f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'ai4se-hse-course-24-25'...\n",
            "remote: Enumerating objects: 22, done.\u001b[K\n",
            "remote: Counting objects: 100% (15/15), done.\u001b[K\n",
            "remote: Compressing objects: 100% (14/14), done.\u001b[K\n",
            "remote: Total 22 (delta 3), reused 1 (delta 1), pack-reused 7 (from 1)\u001b[K\n",
            "Receiving objects: 100% (22/22), 9.56 KiB | 9.56 MiB/s, done.\n",
            "Resolving deltas: 100% (3/3), done.\n"
          ]
        }
      ],
      "source": [
        "#Клонируем данные из репозитория\n",
        "\n",
        "!git clone https://github.com/ai4se-course/ai4se-hse-course-24-25.git"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Устанавливаем необходимые зависимости\n",
        "\n",
        "!pip install -r /content/ai4se-hse-course-24-25/02-func-name-suggestion/requirements.txt"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "2GD7mbmOhQSY",
        "outputId": "7c80d38e-967c-4560-c246-426edd006c1c"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting tree-sitter==0.22.3 (from -r /content/ai4se-hse-course-24-25/02-func-name-suggestion/requirements.txt (line 1))\n",
            "  Downloading tree_sitter-0.22.3-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (10 kB)\n",
            "Collecting tree-sitter-python==0.21.0 (from -r /content/ai4se-hse-course-24-25/02-func-name-suggestion/requirements.txt (line 2))\n",
            "  Downloading tree_sitter_python-0.21.0-cp38-abi3-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (1.8 kB)\n",
            "Collecting datasets==2.20.0 (from -r /content/ai4se-hse-course-24-25/02-func-name-suggestion/requirements.txt (line 3))\n",
            "  Downloading datasets-2.20.0-py3-none-any.whl.metadata (19 kB)\n",
            "Collecting evaluate==0.4.2 (from -r /content/ai4se-hse-course-24-25/02-func-name-suggestion/requirements.txt (line 4))\n",
            "  Downloading evaluate-0.4.2-py3-none-any.whl.metadata (9.3 kB)\n",
            "Collecting rouge_score==0.1.2 (from -r /content/ai4se-hse-course-24-25/02-func-name-suggestion/requirements.txt (line 5))\n",
            "  Downloading rouge_score-0.1.2.tar.gz (17 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: torch==2.5.1 in /usr/local/lib/python3.10/dist-packages (from -r /content/ai4se-hse-course-24-25/02-func-name-suggestion/requirements.txt (line 6)) (2.5.1+cu121)\n",
            "Collecting transformers==4.46.2 (from -r /content/ai4se-hse-course-24-25/02-func-name-suggestion/requirements.txt (line 7))\n",
            "  Downloading transformers-4.46.2-py3-none-any.whl.metadata (44 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m44.1/44.1 kB\u001b[0m \u001b[31m4.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from datasets==2.20.0->-r /content/ai4se-hse-course-24-25/02-func-name-suggestion/requirements.txt (line 3)) (3.16.1)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from datasets==2.20.0->-r /content/ai4se-hse-course-24-25/02-func-name-suggestion/requirements.txt (line 3)) (1.26.4)\n",
            "Requirement already satisfied: pyarrow>=15.0.0 in /usr/local/lib/python3.10/dist-packages (from datasets==2.20.0->-r /content/ai4se-hse-course-24-25/02-func-name-suggestion/requirements.txt (line 3)) (17.0.0)\n",
            "Requirement already satisfied: pyarrow-hotfix in /usr/local/lib/python3.10/dist-packages (from datasets==2.20.0->-r /content/ai4se-hse-course-24-25/02-func-name-suggestion/requirements.txt (line 3)) (0.6)\n",
            "Collecting dill<0.3.9,>=0.3.0 (from datasets==2.20.0->-r /content/ai4se-hse-course-24-25/02-func-name-suggestion/requirements.txt (line 3))\n",
            "  Downloading dill-0.3.8-py3-none-any.whl.metadata (10 kB)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (from datasets==2.20.0->-r /content/ai4se-hse-course-24-25/02-func-name-suggestion/requirements.txt (line 3)) (2.2.2)\n",
            "Requirement already satisfied: requests>=2.32.2 in /usr/local/lib/python3.10/dist-packages (from datasets==2.20.0->-r /content/ai4se-hse-course-24-25/02-func-name-suggestion/requirements.txt (line 3)) (2.32.3)\n",
            "Requirement already satisfied: tqdm>=4.66.3 in /usr/local/lib/python3.10/dist-packages (from datasets==2.20.0->-r /content/ai4se-hse-course-24-25/02-func-name-suggestion/requirements.txt (line 3)) (4.66.6)\n",
            "Collecting xxhash (from datasets==2.20.0->-r /content/ai4se-hse-course-24-25/02-func-name-suggestion/requirements.txt (line 3))\n",
            "  Downloading xxhash-3.5.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (12 kB)\n",
            "Collecting multiprocess (from datasets==2.20.0->-r /content/ai4se-hse-course-24-25/02-func-name-suggestion/requirements.txt (line 3))\n",
            "  Downloading multiprocess-0.70.17-py310-none-any.whl.metadata (7.2 kB)\n",
            "Collecting fsspec<=2024.5.0,>=2023.1.0 (from fsspec[http]<=2024.5.0,>=2023.1.0->datasets==2.20.0->-r /content/ai4se-hse-course-24-25/02-func-name-suggestion/requirements.txt (line 3))\n",
            "  Downloading fsspec-2024.5.0-py3-none-any.whl.metadata (11 kB)\n",
            "Requirement already satisfied: aiohttp in /usr/local/lib/python3.10/dist-packages (from datasets==2.20.0->-r /content/ai4se-hse-course-24-25/02-func-name-suggestion/requirements.txt (line 3)) (3.11.9)\n",
            "Requirement already satisfied: huggingface-hub>=0.21.2 in /usr/local/lib/python3.10/dist-packages (from datasets==2.20.0->-r /content/ai4se-hse-course-24-25/02-func-name-suggestion/requirements.txt (line 3)) (0.26.3)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from datasets==2.20.0->-r /content/ai4se-hse-course-24-25/02-func-name-suggestion/requirements.txt (line 3)) (24.2)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from datasets==2.20.0->-r /content/ai4se-hse-course-24-25/02-func-name-suggestion/requirements.txt (line 3)) (6.0.2)\n",
            "Requirement already satisfied: absl-py in /usr/local/lib/python3.10/dist-packages (from rouge_score==0.1.2->-r /content/ai4se-hse-course-24-25/02-func-name-suggestion/requirements.txt (line 5)) (1.4.0)\n",
            "Requirement already satisfied: nltk in /usr/local/lib/python3.10/dist-packages (from rouge_score==0.1.2->-r /content/ai4se-hse-course-24-25/02-func-name-suggestion/requirements.txt (line 5)) (3.9.1)\n",
            "Requirement already satisfied: six>=1.14.0 in /usr/local/lib/python3.10/dist-packages (from rouge_score==0.1.2->-r /content/ai4se-hse-course-24-25/02-func-name-suggestion/requirements.txt (line 5)) (1.16.0)\n",
            "Requirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.10/dist-packages (from torch==2.5.1->-r /content/ai4se-hse-course-24-25/02-func-name-suggestion/requirements.txt (line 6)) (4.12.2)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch==2.5.1->-r /content/ai4se-hse-course-24-25/02-func-name-suggestion/requirements.txt (line 6)) (3.4.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch==2.5.1->-r /content/ai4se-hse-course-24-25/02-func-name-suggestion/requirements.txt (line 6)) (3.1.4)\n",
            "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.10/dist-packages (from torch==2.5.1->-r /content/ai4se-hse-course-24-25/02-func-name-suggestion/requirements.txt (line 6)) (1.13.1)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers==4.46.2->-r /content/ai4se-hse-course-24-25/02-func-name-suggestion/requirements.txt (line 7)) (2024.9.11)\n",
            "Requirement already satisfied: safetensors>=0.4.1 in /usr/local/lib/python3.10/dist-packages (from transformers==4.46.2->-r /content/ai4se-hse-course-24-25/02-func-name-suggestion/requirements.txt (line 7)) (0.4.5)\n",
            "Requirement already satisfied: tokenizers<0.21,>=0.20 in /usr/local/lib/python3.10/dist-packages (from transformers==4.46.2->-r /content/ai4se-hse-course-24-25/02-func-name-suggestion/requirements.txt (line 7)) (0.20.3)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy==1.13.1->torch==2.5.1->-r /content/ai4se-hse-course-24-25/02-func-name-suggestion/requirements.txt (line 6)) (1.3.0)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets==2.20.0->-r /content/ai4se-hse-course-24-25/02-func-name-suggestion/requirements.txt (line 3)) (2.4.4)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets==2.20.0->-r /content/ai4se-hse-course-24-25/02-func-name-suggestion/requirements.txt (line 3)) (1.3.1)\n",
            "Requirement already satisfied: async-timeout<6.0,>=4.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets==2.20.0->-r /content/ai4se-hse-course-24-25/02-func-name-suggestion/requirements.txt (line 3)) (4.0.3)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets==2.20.0->-r /content/ai4se-hse-course-24-25/02-func-name-suggestion/requirements.txt (line 3)) (24.2.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets==2.20.0->-r /content/ai4se-hse-course-24-25/02-func-name-suggestion/requirements.txt (line 3)) (1.5.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets==2.20.0->-r /content/ai4se-hse-course-24-25/02-func-name-suggestion/requirements.txt (line 3)) (6.1.0)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets==2.20.0->-r /content/ai4se-hse-course-24-25/02-func-name-suggestion/requirements.txt (line 3)) (0.2.1)\n",
            "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets==2.20.0->-r /content/ai4se-hse-course-24-25/02-func-name-suggestion/requirements.txt (line 3)) (1.18.3)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests>=2.32.2->datasets==2.20.0->-r /content/ai4se-hse-course-24-25/02-func-name-suggestion/requirements.txt (line 3)) (3.4.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests>=2.32.2->datasets==2.20.0->-r /content/ai4se-hse-course-24-25/02-func-name-suggestion/requirements.txt (line 3)) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2.32.2->datasets==2.20.0->-r /content/ai4se-hse-course-24-25/02-func-name-suggestion/requirements.txt (line 3)) (2.2.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests>=2.32.2->datasets==2.20.0->-r /content/ai4se-hse-course-24-25/02-func-name-suggestion/requirements.txt (line 3)) (2024.8.30)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch==2.5.1->-r /content/ai4se-hse-course-24-25/02-func-name-suggestion/requirements.txt (line 6)) (3.0.2)\n",
            "INFO: pip is looking at multiple versions of multiprocess to determine which version is compatible with other requirements. This could take a while.\n",
            "Collecting multiprocess (from datasets==2.20.0->-r /content/ai4se-hse-course-24-25/02-func-name-suggestion/requirements.txt (line 3))\n",
            "  Downloading multiprocess-0.70.16-py310-none-any.whl.metadata (7.2 kB)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.10/dist-packages (from nltk->rouge_score==0.1.2->-r /content/ai4se-hse-course-24-25/02-func-name-suggestion/requirements.txt (line 5)) (8.1.7)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.10/dist-packages (from nltk->rouge_score==0.1.2->-r /content/ai4se-hse-course-24-25/02-func-name-suggestion/requirements.txt (line 5)) (1.4.2)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets==2.20.0->-r /content/ai4se-hse-course-24-25/02-func-name-suggestion/requirements.txt (line 3)) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets==2.20.0->-r /content/ai4se-hse-course-24-25/02-func-name-suggestion/requirements.txt (line 3)) (2024.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets==2.20.0->-r /content/ai4se-hse-course-24-25/02-func-name-suggestion/requirements.txt (line 3)) (2024.2)\n",
            "Downloading tree_sitter-0.22.3-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (542 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m542.6/542.6 kB\u001b[0m \u001b[31m18.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading tree_sitter_python-0.21.0-cp38-abi3-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (130 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m130.6/130.6 kB\u001b[0m \u001b[31m6.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading datasets-2.20.0-py3-none-any.whl (547 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m547.8/547.8 kB\u001b[0m \u001b[31m34.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading evaluate-0.4.2-py3-none-any.whl (84 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m84.1/84.1 kB\u001b[0m \u001b[31m8.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading transformers-4.46.2-py3-none-any.whl (10.0 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m10.0/10.0 MB\u001b[0m \u001b[31m69.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading dill-0.3.8-py3-none-any.whl (116 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m116.3/116.3 kB\u001b[0m \u001b[31m9.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading fsspec-2024.5.0-py3-none-any.whl (316 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m316.1/316.1 kB\u001b[0m \u001b[31m23.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading multiprocess-0.70.16-py310-none-any.whl (134 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m134.8/134.8 kB\u001b[0m \u001b[31m12.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading xxhash-3.5.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (194 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m194.1/194.1 kB\u001b[0m \u001b[31m16.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hBuilding wheels for collected packages: rouge_score\n",
            "  Building wheel for rouge_score (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for rouge_score: filename=rouge_score-0.1.2-py3-none-any.whl size=24935 sha256=98dec5b2d9e92675e1d4568324d6ea3799df4397f86cad4540db7c230ff741e2\n",
            "  Stored in directory: /root/.cache/pip/wheels/5f/dd/89/461065a73be61a532ff8599a28e9beef17985c9e9c31e541b4\n",
            "Successfully built rouge_score\n",
            "Installing collected packages: xxhash, tree-sitter-python, tree-sitter, fsspec, dill, rouge_score, multiprocess, transformers, datasets, evaluate\n",
            "  Attempting uninstall: fsspec\n",
            "    Found existing installation: fsspec 2024.10.0\n",
            "    Uninstalling fsspec-2024.10.0:\n",
            "      Successfully uninstalled fsspec-2024.10.0\n",
            "  Attempting uninstall: transformers\n",
            "    Found existing installation: transformers 4.46.3\n",
            "    Uninstalling transformers-4.46.3:\n",
            "      Successfully uninstalled transformers-4.46.3\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "gcsfs 2024.10.0 requires fsspec==2024.10.0, but you have fsspec 2024.5.0 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed datasets-2.20.0 dill-0.3.8 evaluate-0.4.2 fsspec-2024.5.0 multiprocess-0.70.16 rouge_score-0.1.2 transformers-4.46.2 tree-sitter-0.22.3 tree-sitter-python-0.21.0 xxhash-3.5.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Импортируем load_dataset из библиотеки datasets для загрузки данных\n",
        "\n",
        "from datasets import load_dataset\n",
        "\n",
        "dataset = load_dataset('code_search_net', 'python', split='test', trust_remote_code=True)\n",
        "\n",
        "#Выберем первую тысячу записей и переназначим переменную dataset\n",
        "dataset = dataset.select(range(1000))"
      ],
      "metadata": {
        "id": "ILHtW39Shu6O"
      },
      "execution_count": 35,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Выведем полученный датасет\n",
        "\n",
        "print(dataset)\n",
        "\n",
        "#Как видим, датасет содержит 1000 строк и необходимые нам features"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "w846UcuqkGqn",
        "outputId": "d3c7bcaf-3184-4905-c4c7-2f02006d9900"
      },
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Dataset({\n",
            "    features: ['repository_name', 'func_path_in_repository', 'func_name', 'whole_func_string', 'language', 'func_code_string', 'func_code_tokens', 'func_documentation_string', 'func_documentation_tokens', 'split_name', 'func_code_url'],\n",
            "    num_rows: 1000\n",
            "})\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "##Обработка данных\n",
        "\n",
        "#После успешной загрузки данных, их необходимо обработать, выделить названия функций, тела с комментариями и документации и без них\n",
        "\n",
        "from ast import Str\n",
        "from tree_sitter import Language, Parser\n",
        "from tree_sitter_python import language\n",
        "\n",
        "#Утсановим язык python для парсера\n",
        "PY_LANGUAGE = Language(language())\n",
        "\n",
        "# init парсера\n",
        "parser = Parser(PY_LANGUAGE)"
      ],
      "metadata": {
        "id": "yVOtUKPwkNjK"
      },
      "execution_count": 37,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Перейдем к непосредственному написанию обработчика, документация к каждой функции находится непосредственно внутри\n",
        "\n",
        "\n",
        "\n",
        "def extract_function_data(source_code: str):\n",
        "    \"\"\"\n",
        "    Извлекает данные о функции из исходного кода, включая:\n",
        "    - Имя функции.\n",
        "    - Тело функции с комментариями.\n",
        "    - Тело функции без комментариев и строк.\n",
        "\n",
        "    Args:\n",
        "        source_code (str): Исходный код.\n",
        "\n",
        "    Returns:\n",
        "        tuple: (имя функции, тело с комментариями, тело без комментариев).\n",
        "    \"\"\"\n",
        "    tree = parser.parse(source_code.encode('utf8'))\n",
        "    root = tree.root_node\n",
        "\n",
        "    # Инициализация\n",
        "    function_name = None\n",
        "    function_body_with_comments = None\n",
        "    ranges_to_remove = []\n",
        "\n",
        "    # Поиск определения функции\n",
        "    for node in root.children:\n",
        "        if node.type == 'function_definition':\n",
        "            function_name = extract_function_name(node)\n",
        "            function_body_with_comments = extract_node_text(source_code, node.child_by_field_name('body'))\n",
        "            ranges_to_remove += find_removal_ranges(node)\n",
        "\n",
        "    # Удаление комментариев и строк\n",
        "    cleaned_source_code = remove_ranges_from_code(source_code, ranges_to_remove)\n",
        "\n",
        "    # Повторный анализ дерева для извлечения \"чистого\" тела функции\n",
        "    cleaned_tree = parser.parse(cleaned_source_code.encode('utf8'))\n",
        "    root = cleaned_tree.root_node\n",
        "    function_body_without_comments = None\n",
        "\n",
        "    for node in root.children:\n",
        "        if node.type == 'function_definition':\n",
        "            function_body_without_comments = extract_node_text(cleaned_source_code, node.child_by_field_name('body'))\n",
        "\n",
        "    return function_name, function_body_with_comments, function_body_without_comments\n",
        "\n",
        "\n",
        "def extract_function_name(function_node):\n",
        "    \"\"\"\n",
        "    Извлекает имя функции из узла дерева.\n",
        "\n",
        "    Args:\n",
        "        function_node: Узел дерева типа 'function_definition'.\n",
        "\n",
        "    Returns:\n",
        "        str: Имя функции.\n",
        "    \"\"\"\n",
        "    name_node = function_node.child_by_field_name('name')\n",
        "    return name_node.text.decode('utf8') if name_node else None\n",
        "\n",
        "\n",
        "def extract_node_text(source_code: str, node):\n",
        "    \"\"\"\n",
        "    Извлекает текст узла из исходного кода.\n",
        "\n",
        "    Args:\n",
        "        source_code (str): Исходный код.\n",
        "        node: Узел дерева.\n",
        "\n",
        "    Returns:\n",
        "        str: Текст узла.\n",
        "    \"\"\"\n",
        "    if not node:\n",
        "        return None\n",
        "    return source_code[node.start_byte:node.end_byte]\n",
        "\n",
        "\n",
        "def find_removal_ranges(node):\n",
        "    \"\"\"\n",
        "    Находит диапазоны байтов для удаления комментариев и строк.\n",
        "\n",
        "    Args:\n",
        "        node: Узел дерева.\n",
        "\n",
        "    Returns:\n",
        "        list: Список кортежей (start_byte, end_byte) для удаления.\n",
        "    \"\"\"\n",
        "    ranges = []\n",
        "\n",
        "    if node.type == 'comment' or (node.type == 'string' and node.parent.type == 'expression_statement'):\n",
        "        ranges.append((node.start_byte, node.end_byte))\n",
        "\n",
        "    for child in node.children:\n",
        "        ranges += find_removal_ranges(child)\n",
        "\n",
        "    return ranges\n",
        "\n",
        "\n",
        "def remove_ranges_from_code(source_code: str, ranges_to_remove):\n",
        "    \"\"\"\n",
        "    Удаляет указанные диапазоны байтов из исходного кода.\n",
        "\n",
        "    Args:\n",
        "        source_code (str): Исходный код.\n",
        "        ranges_to_remove (list): Список диапазонов (start_byte, end_byte).\n",
        "\n",
        "    Returns:\n",
        "        str: Очищенный исходный код.\n",
        "    \"\"\"\n",
        "    source_code_list = list(source_code)\n",
        "    for start, end in reversed(ranges_to_remove):\n",
        "        del source_code_list[start:end]\n",
        "\n",
        "    # Удаление пустых строк\n",
        "    cleaned_code = ''.join(source_code_list)\n",
        "    return '\\n'.join(line for line in cleaned_code.splitlines() if line.strip())\n",
        "\n"
      ],
      "metadata": {
        "id": "zwBuv6ndkUsb"
      },
      "execution_count": 38,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Проверим как отработает наш код на sample_code из задания:\n",
        "\n",
        "sample_code = \"\"\"\n",
        "def sina_xml_to_url_list(xml_data):\n",
        "    \\\"\\\"\\\"str->list\n",
        "    Convert XML to URL List.\n",
        "    From Biligrab.\n",
        "    \\\"\\\"\\\"\n",
        "    rawurl = []\n",
        "    # Comment1\n",
        "    # Comment 2\n",
        "    dom = parseString(xml_data)\n",
        "    for node in dom.getElementsByTagName('durl'):\n",
        "        url = node.getElementsByTagName('url')[0]  # Comment 3\n",
        "        rawurl.append(url.childNodes[0].data)\n",
        "    return rawurl\n",
        "\"\"\"\n",
        "\n",
        "function_name, function_body_with_comments, function_body_without_comments = extract_function_data(sample_code)\n",
        "\n",
        "print(f\"Function Name: {function_name}\")\n",
        "print(\"Fuction body with comments:\")\n",
        "print(function_body_with_comments)\n",
        "print(\"Function body without comments:\")\n",
        "print(function_body_without_comments)\n",
        "\n",
        "#Отлично! Обработчик работает корректно, теперь можно обучать модель"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "v7UrquCBmlPS",
        "outputId": "f807357e-090b-47d8-a336-c32cfa053b29"
      },
      "execution_count": 39,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Function Name: sina_xml_to_url_list\n",
            "Fuction body with comments:\n",
            "\"\"\"str->list\n",
            "    Convert XML to URL List.\n",
            "    From Biligrab.\n",
            "    \"\"\"\n",
            "    rawurl = []\n",
            "    # Comment1\n",
            "    # Comment 2\n",
            "    dom = parseString(xml_data)\n",
            "    for node in dom.getElementsByTagName('durl'):\n",
            "        url = node.getElementsByTagName('url')[0]  # Comment 3\n",
            "        rawurl.append(url.childNodes[0].data)\n",
            "    return rawurl\n",
            "Function body without comments:\n",
            "rawurl = []\n",
            "    dom = parseString(xml_data)\n",
            "    for node in dom.getElementsByTagName('durl'):\n",
            "        url = node.getElementsByTagName('url')[0]  \n",
            "        rawurl.append(url.childNodes[0].data)\n",
            "    return rawurl\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Прежде чем начнем обучение модели, необходимо обработать искомый датасет\n",
        "\n",
        "def add_elements_to_dataset(dataset):\n",
        "    dataset = dataset.map(lambda x: {\n",
        "        'function_name': extract_function_data(x['whole_func_string'])[0],\n",
        "        'function_body_with_comments': extract_function_data(x['whole_func_string'])[1],\n",
        "        'function_body_without_comments': extract_function_data(x['whole_func_string'])[2]\n",
        "    })\n",
        "    return dataset"
      ],
      "metadata": {
        "id": "SEzHs8nannUz"
      },
      "execution_count": 40,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dataset = add_elements_to_dataset(dataset)\n",
        "\n",
        "print(dataset.column_names)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 87,
          "referenced_widgets": [
            "5aeea738b38f42dc9e678a5e98ed8cb7",
            "cd691d1f563d48c28a832d927c50e723",
            "1ceed22c15614f869955fba038f538f7",
            "7ffdbb1da10f4c46885315be7e0812e6",
            "52d3f4823d2b4792ad5b5f0a3af6f80c",
            "5e0b7e05d1f1462cbbfb38e7d910079b",
            "14ea3cab119c456b88b2334d1ceab03f",
            "c309c635e0a94d2097c3e96ee3952b84",
            "27f6875cfba343e38490984b88f98127",
            "9e9b53fcd9e846a19834aa60372f4f50",
            "865ee319021047438e0e459371f151db"
          ]
        },
        "id": "wUQ959foF251",
        "outputId": "6ded705b-5485-4a14-df80-2bde20d0ad5e"
      },
      "execution_count": 41,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Map:   0%|          | 0/1000 [00:00<?, ? examples/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "5aeea738b38f42dc9e678a5e98ed8cb7"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['repository_name', 'func_path_in_repository', 'func_name', 'whole_func_string', 'language', 'func_code_string', 'func_code_tokens', 'func_documentation_string', 'func_documentation_tokens', 'split_name', 'func_code_url', 'function_name', 'function_body_with_comments', 'function_body_without_comments']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Отлично, датасет обработан успешно, столбцы добавлены. Теперь выберем несколько рандомных функций из датасета и проверим, что получилось:\n",
        "import random\n",
        "\n",
        "start = random.randint(1, 995)\n",
        "end = start + 5\n",
        "\n",
        "for i in range(start, end):\n",
        "  print(f\"whole_func_string:\\n{dataset[i]['whole_func_string']}\")\n",
        "  print(f\"function_name:\\n{dataset[i]['function_name']}\")\n",
        "  print(f\"function_body_with_comments:\\n{dataset[i]['function_body_with_comments']}\")\n",
        "  print(f\"function_body_without_comments:\\n{dataset[i]['function_body_without_comments']}\")\n",
        "  print('__________________________________________________________________________________________')\n",
        "  print('__________________________________________________________________________________________')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rnIkrB01IYpv",
        "outputId": "a2e48357-6d47-4c42-dd6b-f74768f1e963"
      },
      "execution_count": 42,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "whole_func_string:\n",
            "def render_log_filename(ti, try_number, filename_template):\n",
            "    \"\"\"\n",
            "    Given task instance, try_number, filename_template, return the rendered log\n",
            "    filename\n",
            "\n",
            "    :param ti: task instance\n",
            "    :param try_number: try_number of the task\n",
            "    :param filename_template: filename template, which can be jinja template or\n",
            "        python string template\n",
            "    \"\"\"\n",
            "    filename_template, filename_jinja_template = parse_template_string(filename_template)\n",
            "    if filename_jinja_template:\n",
            "        jinja_context = ti.get_template_context()\n",
            "        jinja_context['try_number'] = try_number\n",
            "        return filename_jinja_template.render(**jinja_context)\n",
            "\n",
            "    return filename_template.format(dag_id=ti.dag_id,\n",
            "                                    task_id=ti.task_id,\n",
            "                                    execution_date=ti.execution_date.isoformat(),\n",
            "                                    try_number=try_number)\n",
            "function_name:\n",
            "render_log_filename\n",
            "function_body_with_comments:\n",
            "\"\"\"\n",
            "    Given task instance, try_number, filename_template, return the rendered log\n",
            "    filename\n",
            "\n",
            "    :param ti: task instance\n",
            "    :param try_number: try_number of the task\n",
            "    :param filename_template: filename template, which can be jinja template or\n",
            "        python string template\n",
            "    \"\"\"\n",
            "    filename_template, filename_jinja_template = parse_template_string(filename_template)\n",
            "    if filename_jinja_template:\n",
            "        jinja_context = ti.get_template_context()\n",
            "        jinja_context['try_number'] = try_number\n",
            "        return filename_jinja_template.render(**jinja_context)\n",
            "\n",
            "    return filename_template.format(dag_id=ti.dag_id,\n",
            "                                    task_id=ti.task_id,\n",
            "                                    execution_date=ti.execution_date.isoformat(),\n",
            "                                    try_number=try_number)\n",
            "function_body_without_comments:\n",
            "filename_template, filename_jinja_template = parse_template_string(filename_template)\n",
            "    if filename_jinja_template:\n",
            "        jinja_context = ti.get_template_context()\n",
            "        jinja_context['try_number'] = try_number\n",
            "        return filename_jinja_template.render(**jinja_context)\n",
            "    return filename_template.format(dag_id=ti.dag_id,\n",
            "                                    task_id=ti.task_id,\n",
            "                                    execution_date=ti.execution_date.isoformat(),\n",
            "                                    try_number=try_number)\n",
            "__________________________________________________________________________________________\n",
            "__________________________________________________________________________________________\n",
            "whole_func_string:\n",
            "def get_task_instance(dag_id, task_id, execution_date):\n",
            "    \"\"\"Return the task object identified by the given dag_id and task_id.\"\"\"\n",
            "\n",
            "    dagbag = DagBag()\n",
            "\n",
            "    # Check DAG exists.\n",
            "    if dag_id not in dagbag.dags:\n",
            "        error_message = \"Dag id {} not found\".format(dag_id)\n",
            "        raise DagNotFound(error_message)\n",
            "\n",
            "    # Get DAG object and check Task Exists\n",
            "    dag = dagbag.get_dag(dag_id)\n",
            "    if not dag.has_task(task_id):\n",
            "        error_message = 'Task {} not found in dag {}'.format(task_id, dag_id)\n",
            "        raise TaskNotFound(error_message)\n",
            "\n",
            "    # Get DagRun object and check that it exists\n",
            "    dagrun = dag.get_dagrun(execution_date=execution_date)\n",
            "    if not dagrun:\n",
            "        error_message = ('Dag Run for date {} not found in dag {}'\n",
            "                         .format(execution_date, dag_id))\n",
            "        raise DagRunNotFound(error_message)\n",
            "\n",
            "    # Get task instance object and check that it exists\n",
            "    task_instance = dagrun.get_task_instance(task_id)\n",
            "    if not task_instance:\n",
            "        error_message = ('Task {} instance for date {} not found'\n",
            "                         .format(task_id, execution_date))\n",
            "        raise TaskInstanceNotFound(error_message)\n",
            "\n",
            "    return task_instance\n",
            "function_name:\n",
            "get_task_instance\n",
            "function_body_with_comments:\n",
            "\"\"\"Return the task object identified by the given dag_id and task_id.\"\"\"\n",
            "\n",
            "    dagbag = DagBag()\n",
            "\n",
            "    # Check DAG exists.\n",
            "    if dag_id not in dagbag.dags:\n",
            "        error_message = \"Dag id {} not found\".format(dag_id)\n",
            "        raise DagNotFound(error_message)\n",
            "\n",
            "    # Get DAG object and check Task Exists\n",
            "    dag = dagbag.get_dag(dag_id)\n",
            "    if not dag.has_task(task_id):\n",
            "        error_message = 'Task {} not found in dag {}'.format(task_id, dag_id)\n",
            "        raise TaskNotFound(error_message)\n",
            "\n",
            "    # Get DagRun object and check that it exists\n",
            "    dagrun = dag.get_dagrun(execution_date=execution_date)\n",
            "    if not dagrun:\n",
            "        error_message = ('Dag Run for date {} not found in dag {}'\n",
            "                         .format(execution_date, dag_id))\n",
            "        raise DagRunNotFound(error_message)\n",
            "\n",
            "    # Get task instance object and check that it exists\n",
            "    task_instance = dagrun.get_task_instance(task_id)\n",
            "    if not task_instance:\n",
            "        error_message = ('Task {} instance for date {} not found'\n",
            "                         .format(task_id, execution_date))\n",
            "        raise TaskInstanceNotFound(error_message)\n",
            "\n",
            "    return task_instance\n",
            "function_body_without_comments:\n",
            "dagbag = DagBag()\n",
            "    if dag_id not in dagbag.dags:\n",
            "        error_message = \"Dag id {} not found\".format(dag_id)\n",
            "        raise DagNotFound(error_message)\n",
            "    dag = dagbag.get_dag(dag_id)\n",
            "    if not dag.has_task(task_id):\n",
            "        error_message = 'Task {} not found in dag {}'.format(task_id, dag_id)\n",
            "        raise TaskNotFound(error_message)\n",
            "    dagrun = dag.get_dagrun(execution_date=execution_date)\n",
            "    if not dagrun:\n",
            "        error_message = ('Dag Run for date {} not found in dag {}'\n",
            "                         .format(execution_date, dag_id))\n",
            "        raise DagRunNotFound(error_message)\n",
            "    task_instance = dagrun.get_task_instance(task_id)\n",
            "    if not task_instance:\n",
            "        error_message = ('Task {} instance for date {} not found'\n",
            "                         .format(task_id, execution_date))\n",
            "        raise TaskInstanceNotFound(error_message)\n",
            "    return task_instance\n",
            "__________________________________________________________________________________________\n",
            "__________________________________________________________________________________________\n",
            "whole_func_string:\n",
            "def _integrate_plugins():\n",
            "    \"\"\"Integrate plugins to the context\"\"\"\n",
            "    import sys\n",
            "    from airflow.plugins_manager import operators_modules\n",
            "    for operators_module in operators_modules:\n",
            "        sys.modules[operators_module.__name__] = operators_module\n",
            "        globals()[operators_module._name] = operators_module\n",
            "function_name:\n",
            "_integrate_plugins\n",
            "function_body_with_comments:\n",
            "\"\"\"Integrate plugins to the context\"\"\"\n",
            "    import sys\n",
            "    from airflow.plugins_manager import operators_modules\n",
            "    for operators_module in operators_modules:\n",
            "        sys.modules[operators_module.__name__] = operators_module\n",
            "        globals()[operators_module._name] = operators_module\n",
            "function_body_without_comments:\n",
            "import sys\n",
            "    from airflow.plugins_manager import operators_modules\n",
            "    for operators_module in operators_modules:\n",
            "        sys.modules[operators_module.__name__] = operators_module\n",
            "        globals()[operators_module._name] = operators_module\n",
            "__________________________________________________________________________________________\n",
            "__________________________________________________________________________________________\n",
            "whole_func_string:\n",
            "def get_conn(self):\n",
            "        \"\"\"Returns a Google Cloud Dataproc service object.\"\"\"\n",
            "        http_authorized = self._authorize()\n",
            "        return build(\n",
            "            'dataproc', self.api_version, http=http_authorized,\n",
            "            cache_discovery=False)\n",
            "function_name:\n",
            "get_conn\n",
            "function_body_with_comments:\n",
            "\"\"\"Returns a Google Cloud Dataproc service object.\"\"\"\n",
            "        http_authorized = self._authorize()\n",
            "        return build(\n",
            "            'dataproc', self.api_version, http=http_authorized,\n",
            "            cache_discovery=False)\n",
            "function_body_without_comments:\n",
            "http_authorized = self._authorize()\n",
            "        return build(\n",
            "            'dataproc', self.api_version, http=http_authorized,\n",
            "            cache_discovery=False)\n",
            "__________________________________________________________________________________________\n",
            "__________________________________________________________________________________________\n",
            "whole_func_string:\n",
            "def wait(self, operation):\n",
            "        \"\"\"Awaits for Google Cloud Dataproc Operation to complete.\"\"\"\n",
            "        submitted = _DataProcOperation(self.get_conn(), operation,\n",
            "                                       self.num_retries)\n",
            "        submitted.wait_for_done()\n",
            "function_name:\n",
            "wait\n",
            "function_body_with_comments:\n",
            "\"\"\"Awaits for Google Cloud Dataproc Operation to complete.\"\"\"\n",
            "        submitted = _DataProcOperation(self.get_conn(), operation,\n",
            "                                       self.num_retries)\n",
            "        submitted.wait_for_done()\n",
            "function_body_without_comments:\n",
            "submitted = _DataProcOperation(self.get_conn(), operation,\n",
            "                                       self.num_retries)\n",
            "        submitted.wait_for_done()\n",
            "__________________________________________________________________________________________\n",
            "__________________________________________________________________________________________\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Теперь запустим обучение через T5\n",
        "\n",
        "\n",
        "from transformers import T5ForConditionalGeneration, AutoTokenizer\n",
        "import torch\n",
        "\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "model_name = \"Salesforce/codet5p-220m\"\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "model = T5ForConditionalGeneration.from_pretrained(model_name).to(device)\n",
        "model.eval()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OUMS2-SMKx1t",
        "outputId": "dc561f97-4dda-4a66-a8e7-d53712d27569"
      },
      "execution_count": 43,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "T5ForConditionalGeneration(\n",
              "  (shared): Embedding(32100, 768)\n",
              "  (encoder): T5Stack(\n",
              "    (embed_tokens): Embedding(32100, 768)\n",
              "    (block): ModuleList(\n",
              "      (0): T5Block(\n",
              "        (layer): ModuleList(\n",
              "          (0): T5LayerSelfAttention(\n",
              "            (SelfAttention): T5Attention(\n",
              "              (q): Linear(in_features=768, out_features=768, bias=False)\n",
              "              (k): Linear(in_features=768, out_features=768, bias=False)\n",
              "              (v): Linear(in_features=768, out_features=768, bias=False)\n",
              "              (o): Linear(in_features=768, out_features=768, bias=False)\n",
              "              (relative_attention_bias): Embedding(32, 12)\n",
              "            )\n",
              "            (layer_norm): T5LayerNorm()\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "          (1): T5LayerFF(\n",
              "            (DenseReluDense): T5DenseActDense(\n",
              "              (wi): Linear(in_features=768, out_features=3072, bias=False)\n",
              "              (wo): Linear(in_features=3072, out_features=768, bias=False)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "              (act): ReLU()\n",
              "            )\n",
              "            (layer_norm): T5LayerNorm()\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "      )\n",
              "      (1-11): 11 x T5Block(\n",
              "        (layer): ModuleList(\n",
              "          (0): T5LayerSelfAttention(\n",
              "            (SelfAttention): T5Attention(\n",
              "              (q): Linear(in_features=768, out_features=768, bias=False)\n",
              "              (k): Linear(in_features=768, out_features=768, bias=False)\n",
              "              (v): Linear(in_features=768, out_features=768, bias=False)\n",
              "              (o): Linear(in_features=768, out_features=768, bias=False)\n",
              "            )\n",
              "            (layer_norm): T5LayerNorm()\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "          (1): T5LayerFF(\n",
              "            (DenseReluDense): T5DenseActDense(\n",
              "              (wi): Linear(in_features=768, out_features=3072, bias=False)\n",
              "              (wo): Linear(in_features=3072, out_features=768, bias=False)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "              (act): ReLU()\n",
              "            )\n",
              "            (layer_norm): T5LayerNorm()\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "      )\n",
              "    )\n",
              "    (final_layer_norm): T5LayerNorm()\n",
              "    (dropout): Dropout(p=0.1, inplace=False)\n",
              "  )\n",
              "  (decoder): T5Stack(\n",
              "    (embed_tokens): Embedding(32100, 768)\n",
              "    (block): ModuleList(\n",
              "      (0): T5Block(\n",
              "        (layer): ModuleList(\n",
              "          (0): T5LayerSelfAttention(\n",
              "            (SelfAttention): T5Attention(\n",
              "              (q): Linear(in_features=768, out_features=768, bias=False)\n",
              "              (k): Linear(in_features=768, out_features=768, bias=False)\n",
              "              (v): Linear(in_features=768, out_features=768, bias=False)\n",
              "              (o): Linear(in_features=768, out_features=768, bias=False)\n",
              "              (relative_attention_bias): Embedding(32, 12)\n",
              "            )\n",
              "            (layer_norm): T5LayerNorm()\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "          (1): T5LayerCrossAttention(\n",
              "            (EncDecAttention): T5Attention(\n",
              "              (q): Linear(in_features=768, out_features=768, bias=False)\n",
              "              (k): Linear(in_features=768, out_features=768, bias=False)\n",
              "              (v): Linear(in_features=768, out_features=768, bias=False)\n",
              "              (o): Linear(in_features=768, out_features=768, bias=False)\n",
              "            )\n",
              "            (layer_norm): T5LayerNorm()\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "          (2): T5LayerFF(\n",
              "            (DenseReluDense): T5DenseActDense(\n",
              "              (wi): Linear(in_features=768, out_features=3072, bias=False)\n",
              "              (wo): Linear(in_features=3072, out_features=768, bias=False)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "              (act): ReLU()\n",
              "            )\n",
              "            (layer_norm): T5LayerNorm()\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "      )\n",
              "      (1-11): 11 x T5Block(\n",
              "        (layer): ModuleList(\n",
              "          (0): T5LayerSelfAttention(\n",
              "            (SelfAttention): T5Attention(\n",
              "              (q): Linear(in_features=768, out_features=768, bias=False)\n",
              "              (k): Linear(in_features=768, out_features=768, bias=False)\n",
              "              (v): Linear(in_features=768, out_features=768, bias=False)\n",
              "              (o): Linear(in_features=768, out_features=768, bias=False)\n",
              "            )\n",
              "            (layer_norm): T5LayerNorm()\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "          (1): T5LayerCrossAttention(\n",
              "            (EncDecAttention): T5Attention(\n",
              "              (q): Linear(in_features=768, out_features=768, bias=False)\n",
              "              (k): Linear(in_features=768, out_features=768, bias=False)\n",
              "              (v): Linear(in_features=768, out_features=768, bias=False)\n",
              "              (o): Linear(in_features=768, out_features=768, bias=False)\n",
              "            )\n",
              "            (layer_norm): T5LayerNorm()\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "          (2): T5LayerFF(\n",
              "            (DenseReluDense): T5DenseActDense(\n",
              "              (wi): Linear(in_features=768, out_features=3072, bias=False)\n",
              "              (wo): Linear(in_features=3072, out_features=768, bias=False)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "              (act): ReLU()\n",
              "            )\n",
              "            (layer_norm): T5LayerNorm()\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "      )\n",
              "    )\n",
              "    (final_layer_norm): T5LayerNorm()\n",
              "    (dropout): Dropout(p=0.1, inplace=False)\n",
              "  )\n",
              "  (lm_head): Linear(in_features=768, out_features=32100, bias=False)\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 43
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(dataset[1])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "q0GGP6LseowQ",
        "outputId": "d0ab8638-b105-498c-fcad-6f1f80cc5a4b"
      },
      "execution_count": 44,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'repository_name': 'soimort/you-get', 'func_path_in_repository': 'src/you_get/extractors/miomio.py', 'func_name': 'sina_xml_to_url_list', 'whole_func_string': 'def sina_xml_to_url_list(xml_data):\\n    \"\"\"str->list\\n    Convert XML to URL List.\\n    From Biligrab.\\n    \"\"\"\\n    rawurl = []\\n    dom = parseString(xml_data)\\n    for node in dom.getElementsByTagName(\\'durl\\'):\\n        url = node.getElementsByTagName(\\'url\\')[0]\\n        rawurl.append(url.childNodes[0].data)\\n    return rawurl', 'language': 'python', 'func_code_string': 'def sina_xml_to_url_list(xml_data):\\n    \"\"\"str->list\\n    Convert XML to URL List.\\n    From Biligrab.\\n    \"\"\"\\n    rawurl = []\\n    dom = parseString(xml_data)\\n    for node in dom.getElementsByTagName(\\'durl\\'):\\n        url = node.getElementsByTagName(\\'url\\')[0]\\n        rawurl.append(url.childNodes[0].data)\\n    return rawurl', 'func_code_tokens': ['def', 'sina_xml_to_url_list', '(', 'xml_data', ')', ':', 'rawurl', '=', '[', ']', 'dom', '=', 'parseString', '(', 'xml_data', ')', 'for', 'node', 'in', 'dom', '.', 'getElementsByTagName', '(', \"'durl'\", ')', ':', 'url', '=', 'node', '.', 'getElementsByTagName', '(', \"'url'\", ')', '[', '0', ']', 'rawurl', '.', 'append', '(', 'url', '.', 'childNodes', '[', '0', ']', '.', 'data', ')', 'return', 'rawurl'], 'func_documentation_string': 'str->list\\n    Convert XML to URL List.\\n    From Biligrab.', 'func_documentation_tokens': ['str', '-', '>', 'list', 'Convert', 'XML', 'to', 'URL', 'List', '.', 'From', 'Biligrab', '.'], 'split_name': 'test', 'func_code_url': 'https://github.com/soimort/you-get/blob/b746ac01c9f39de94cac2d56f665285b0523b974/src/you_get/extractors/miomio.py#L41-L51', 'function_name': 'sina_xml_to_url_list', 'function_body_with_comments': '\"\"\"str->list\\n    Convert XML to URL List.\\n    From Biligrab.\\n    \"\"\"\\n    rawurl = []\\n    dom = parseString(xml_data)\\n    for node in dom.getElementsByTagName(\\'durl\\'):\\n        url = node.getElementsByTagName(\\'url\\')[0]\\n        rawurl.append(url.childNodes[0].data)\\n    return rawurl', 'function_body_without_comments': \"rawurl = []\\n    dom = parseString(xml_data)\\n    for node in dom.getElementsByTagName('durl'):\\n        url = node.getElementsByTagName('url')[0]\\n        rawurl.append(url.childNodes[0].data)\\n    return rawurl\"}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from collections.abc import Iterable\n",
        "from functools import cache\n",
        "from pprint import pprint\n",
        "\n",
        "import evaluate\n",
        "import datasets\n",
        "\n",
        "\n",
        "@cache\n",
        "def _init_metrics():\n",
        "    return (evaluate.load(\"exact_match\"), evaluate.load(\"rouge\"))\n",
        "\n",
        "\n",
        "def predict(dataset: datasets.Dataset, model, tokenizer) -> None:\n",
        "    \"\"\"\n",
        "    Предсказывает имена функций и оценивает результаты.\n",
        "    \"\"\"\n",
        "    predictions = [\n",
        "        model_predict(f\"def :\\n    {example['function_body_without_comments']}\", model, tokenizer)\n",
        "        for example in dataset\n",
        "    ]\n",
        "    references = [example[\"function_name\"] for example in dataset]\n",
        "\n",
        "    # Нормализация\n",
        "    predictions = [p.strip().lower() for p in predictions]\n",
        "    references = [r.strip().lower() for r in references]\n",
        "\n",
        "    # Оценка метрик\n",
        "    eval_results = run_evaluate(predictions=predictions, references=references)\n",
        "\n",
        "    # Вывод результатов\n",
        "    print()\n",
        "    print(\"*\" * 80)\n",
        "    print(\"Evaluation results:\")\n",
        "    pprint(eval_results)\n",
        "    print(\"*\" * 80)\n",
        "    print()\n",
        "\n",
        "\n",
        "def model_predict(input_text: str, model, tokenizer) -> str:\n",
        "    \"\"\"\n",
        "    Выполняет предсказание имени функции с использованием модели.\n",
        "\n",
        "    Args:\n",
        "        input_text (str): Входной текст (тело функции).\n",
        "        model: Предобученная модель.\n",
        "        tokenizer: Токенизатор модели.\n",
        "\n",
        "    Returns:\n",
        "        str: Предсказанное имя функции.\n",
        "    \"\"\"\n",
        "    # Исправлено: используем input_text\n",
        "    input_ids = tokenizer(input_text, return_tensors=\"pt\", padding=True, truncation=True).input_ids.to(device)\n",
        "    with torch.no_grad():  # Отключение вычисления градиентов\n",
        "        outputs = model.generate(input_ids, max_length=20)\n",
        "    prediction = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
        "\n",
        "    # Очистка предсказания\n",
        "    if '(' in prediction:\n",
        "        prediction = prediction.split('(')[0]\n",
        "    prediction = prediction.strip()\n",
        "    if ' ' in prediction:\n",
        "        splitted = prediction.split(' ')\n",
        "        prediction = splitted[1] if splitted[0] == 'def' else splitted[0]\n",
        "    return prediction\n",
        "\n",
        "\n",
        "def run_evaluate(\n",
        "    predictions: Iterable[str], references: Iterable[str]\n",
        ") -> dict[str, float]:\n",
        "    \"\"\"\n",
        "    Оценивает метрики Exact Match и ROUGE.\n",
        "\n",
        "    Args:\n",
        "        predictions (Iterable[str]): Предсказанные имена функций.\n",
        "        references (Iterable[str]): Эталонные имена функций.\n",
        "\n",
        "    Returns:\n",
        "        dict[str, float]: Результаты оценки.\n",
        "    \"\"\"\n",
        "    em, rouge = _init_metrics()\n",
        "    em_score = em.compute(predictions=predictions, references=references)\n",
        "    rouge_scores = rouge.compute(predictions=predictions, references=references)\n",
        "\n",
        "    return {**rouge_scores, **em_score}\n",
        "\n",
        "\n",
        "# Предсказание и оценка\n",
        "predict(dataset, model=model, tokenizer=tokenizer)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Cz_gR7wjyfUe",
        "outputId": "32cf334b-61eb-4915-8295-4f814cd63d61"
      },
      "execution_count": 47,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "********************************************************************************\n",
            "Evaluation results:\n",
            "{'exact_match': 0.002,\n",
            " 'rouge1': 0.04656150793650793,\n",
            " 'rouge2': 0.01610357142857143,\n",
            " 'rougeL': 0.04687460317460315,\n",
            " 'rougeLsum': 0.04671071428571427}\n",
            "********************************************************************************\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Теперь аналогичная процедура для тела с комментариями:\n",
        "\n",
        "from collections.abc import Iterable\n",
        "from functools import cache\n",
        "from pprint import pprint\n",
        "\n",
        "import evaluate\n",
        "import datasets\n",
        "\n",
        "\n",
        "@cache\n",
        "def _init_metrics():\n",
        "    return (evaluate.load(\"exact_match\"), evaluate.load(\"rouge\"))\n",
        "\n",
        "\n",
        "def predict(dataset: datasets.Dataset, model, tokenizer) -> None:\n",
        "    \"\"\"\n",
        "    Предсказывает имена функций и оценивает результаты.\n",
        "    \"\"\"\n",
        "    predictions = [\n",
        "        model_predict(f\"def :\\n    {example['function_body_with_comments']}\", model, tokenizer)\n",
        "        for example in dataset\n",
        "    ]\n",
        "    references = [example[\"function_name\"] for example in dataset]\n",
        "\n",
        "    # Нормализация\n",
        "    predictions = [p.strip().lower() for p in predictions]\n",
        "    references = [r.strip().lower() for r in references]\n",
        "\n",
        "    # Оценка метрик\n",
        "    eval_results = run_evaluate(predictions=predictions, references=references)\n",
        "\n",
        "    # Вывод результатов\n",
        "    print()\n",
        "    print(\"*\" * 80)\n",
        "    print(\"Evaluation results:\")\n",
        "    pprint(eval_results)\n",
        "    print(\"*\" * 80)\n",
        "    print()\n",
        "\n",
        "\n",
        "def model_predict(input_text: str, model, tokenizer) -> str:\n",
        "    \"\"\"\n",
        "    Выполняет предсказание имени функции с использованием модели.\n",
        "\n",
        "    Args:\n",
        "        input_text (str): Входной текст (тело функции).\n",
        "        model: Предобученная модель.\n",
        "        tokenizer: Токенизатор модели.\n",
        "\n",
        "    Returns:\n",
        "        str: Предсказанное имя функции.\n",
        "    \"\"\"\n",
        "    # Исправлено: используем input_text\n",
        "    input_ids = tokenizer(input_text, return_tensors=\"pt\", padding=True, truncation=True).input_ids.to(device)\n",
        "    with torch.no_grad():  # Отключение вычисления градиентов\n",
        "        outputs = model.generate(input_ids, max_length=20)\n",
        "    prediction = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
        "\n",
        "    # Очистка предсказания\n",
        "    if '(' in prediction:\n",
        "        prediction = prediction.split('(')[0]\n",
        "    prediction = prediction.strip()\n",
        "    if ' ' in prediction:\n",
        "        splitted = prediction.split(' ')\n",
        "        prediction = splitted[1] if splitted[0] == 'def' else splitted[0]\n",
        "    return prediction\n",
        "\n",
        "\n",
        "def run_evaluate(\n",
        "    predictions: Iterable[str], references: Iterable[str]\n",
        ") -> dict[str, float]:\n",
        "    \"\"\"\n",
        "    Оценивает метрики Exact Match и ROUGE.\n",
        "\n",
        "    Args:\n",
        "        predictions (Iterable[str]): Предсказанные имена функций.\n",
        "        references (Iterable[str]): Эталонные имена функций.\n",
        "\n",
        "    Returns:\n",
        "        dict[str, float]: Результаты оценки.\n",
        "    \"\"\"\n",
        "    em, rouge = _init_metrics()\n",
        "    em_score = em.compute(predictions=predictions, references=references)\n",
        "    rouge_scores = rouge.compute(predictions=predictions, references=references)\n",
        "\n",
        "    return {**rouge_scores, **em_score}\n",
        "\n",
        "\n",
        "# Предсказание и оценка\n",
        "predict(dataset, model=model, tokenizer=tokenizer)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4LVfjFGZko6u",
        "outputId": "e7ea6dc4-1327-44d6-ac68-6f3a1bacf53b"
      },
      "execution_count": 49,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "********************************************************************************\n",
            "Evaluation results:\n",
            "{'exact_match': 0.017,\n",
            " 'rouge1': 0.07696179653679651,\n",
            " 'rouge2': 0.027938095238095238,\n",
            " 'rougeL': 0.07709534632034631,\n",
            " 'rougeLsum': 0.07669058441558442}\n",
            "********************************************************************************\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "XNWnPG1jkvnY"
      },
      "execution_count": 48,
      "outputs": []
    }
  ]
}